from transformers import pipeline
from PIL import Image
import numpy as np
import cv2
import os
import torch
from tqdm import tqdm

try:
    from ultralytics import YOLO

    YOLO_AVAILABLE = True
except ImportError:
    YOLO_AVAILABLE = False
    print("âš  Warning: YOLO not available, using basic detection only")


class EnhancedContentDetector:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        try:
            self.nsfw_model = pipeline(
                "image-classification",
                model="Falconsai/nsfw_image_detection",
                device=self.device
            )
        except Exception as e:
            print(f"âš  Failed to load NSFW model: {str(e)}")
            self.nsfw_model = None

        self.object_model = None
        if YOLO_AVAILABLE:
            try:
                self.object_model = YOLO("yolov8n.pt")
                print("âœ… YOLO model loaded successfully")
            except Exception as e:
                print(f"âš  Failed to load YOLO model: {str(e)}")

        self.danger_categories = {
            "drugs": ["syringe", "pill", "joint", "pipe", "drug"],
            "weapons": ["gun", "knife", "rifle", "pistol", "weapon"],
            "violence": ["blood", "corpse", "handcuffs", "fight"]
        }

        # ÐšÐ»Ð°ÑÑÑ‹ YOLO, ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð»ÑŽÐ´ÑÐ¼
        self.person_classes = ["person"]

        self.nsfw_threshold = 0.7
        self.object_confidence = 0.6
        self.person_confidence = 0.5  # ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ð´Ð»Ñ Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ°

    def detect_objects(self, image):
        if not self.object_model:
            return []

        try:
            results = self.object_model(image)
            detected = []

            for result in results:
                for box in result.boxes:
                    obj_class = self.object_model.names[int(box.cls)]
                    conf = float(box.conf)
                    if conf > self.object_confidence:
                        detected.append((obj_class.lower(), conf))
            return detected
        except Exception as e:
            print(f"âš  Object detection error: {str(e)}")
            return []

    def contains_person(self, detected_objects):
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°, ÐµÑÑ‚ÑŒ Ð»Ð¸ Ð½Ð° Ñ„Ð¾Ñ‚Ð¾ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº"""
        for obj, conf in detected_objects:
            if obj in self.person_classes and conf >= self.person_confidence:
                return True, f"{obj} ({conf:.2f})"
        return False, None

    def analyze_content(self, detected_objects):
        violations = {category: False for category in self.danger_categories}
        dangerous_items = []

        for obj, conf in detected_objects:
            for category, keywords in self.danger_categories.items():
                if any(keyword in obj for keyword in keywords):
                    violations[category] = True
                    dangerous_items.append(f"{obj} ({conf:.2f})")
                    break

        return violations, dangerous_items

    def analyze_image(self, image_path):
        try:
            if not os.path.exists(image_path):
                return {"error": "File not found"}

            img = Image.open(image_path)
            img_cv = cv2.imread(image_path)

            result = {
                "file": os.path.basename(image_path),
                "verdict": "ðŸŸ¢ CLEAN",
                "violations": {},
                "details": {},
                "contains_person": False,
                "person_details": None
            }

            # Ð”ÐµÑ‚ÐµÐºÑ†Ð¸Ñ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² ÐµÑÐ»Ð¸ YOLO Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½
            detected_objects = []
            if self.object_model:
                try:
                    detected_objects = self.detect_objects(img_cv)
                    result['details']['detected_objects'] = detected_objects

                    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ°
                    has_person, person_info = self.contains_person(detected_objects)
                    result['contains_person'] = has_person
                    if has_person:
                        result['person_details'] = person_info
                    else:
                        result['verdict'] = "ðŸ”´ NO PERSON"
                        return result

                    # ÐÐ½Ð°Ð»Ð¸Ð· Ð¾Ð¿Ð°ÑÐ½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°
                    object_violations, dangerous_items = self.analyze_content(detected_objects)
                    for cat, detected in object_violations.items():
                        if detected:
                            result['violations'][cat] = True

                    if dangerous_items:
                        result['details']['dangerous_items'] = dangerous_items
                except Exception as e:
                    print(f"âš  Object detection error: {str(e)}")

            # ÐÐ½Ð°Ð»Ð¸Ð· NSFW ÐµÑÐ»Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð° Ð¸ ÐµÑÑ‚ÑŒ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐº Ð½Ð° Ñ„Ð¾Ñ‚Ð¾
            if self.nsfw_model and result['contains_person']:
                try:
                    nsfw_results = self.nsfw_model(img)
                    nsfw_score = next((r['score'] for r in nsfw_results if r['label'] == 'nsfw'), 0.0)
                    result['details']['nsfw_score'] = f"{nsfw_score * 100:.1f}%"
                    if nsfw_score > self.nsfw_threshold:
                        result['violations']['nudity'] = True
                except Exception as e:
                    print(f"âš  NSFW analysis error: {str(e)}")

            if any(result['violations'].values()):
                result['verdict'] = "ðŸ”´ BANNED"

            return result

        except Exception as e:
            return {
                "file": os.path.basename(image_path),
                "error": str(e)
            }
